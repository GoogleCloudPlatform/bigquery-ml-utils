{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8023ade1"
      },
      "source": [
        "# Notebook Summary\n",
        "\n",
        "This notebook is designed to automate the process of training and evaluating ARIMA_PLUS models on Monash datasets. It calculates the Mean Absolute Scaled Error (MASE) for each model and logs the results to a Google Sheet.\n",
        "\n",
        "\n",
        "1.  **Input Parameters:** Modify the `auto_arima_max_order_list_string` and `max_time_series_length_list_string` in the first code cell to specify the ARIMA orders and time series lengths you want to test.\n",
        "2.  **Google Sheet Setup:** Ensure the Google Sheet specified by `google_sheet_name` and `worksheet_name` exists and that you have the necessary permissions to write to it.\n",
        "3.  **Dataset Names:** The notebook uses predefined lists of dataset names (`dataset_names` and `dataset_names_v2`). If you have different datasets, update these lists accordingly.\n",
        "4.  **BigQuery Project:** Make sure the `project` variable in the first code cell is set to your BigQuery project ID.\n",
        "5.  **Run All:** After setting up the parameters and ensuring the Google Sheet exists, run all cells in the notebook (`Run all`). The notebook will iterate through the specified datasets, train models with different parameters, calculate MASE, and append the results to your Google Sheet.\n",
        "6.  **Monitor Progress:** The notebook prints progress updates to the console, indicating which dataset and parameters are currently being processed.\n",
        "7.  **Review Results:** Once the execution is complete, check your Google Sheet for the MASE scores and other details for each model."
      ]
    },
    {
      "metadata": {
        "id": "thU4SE2V7QgB"
      },
      "cell_type": "markdown",
      "source": [
        "# Set up parameters and authorize bigquery and google sheet client\n"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "XJw0ANr6VYZR"
      },
      "cell_type": "code",
      "source": [
        "import gspread\n",
        "from google.colab import auth\n",
        "from google.cloud import bigquery\n",
        "from google.auth import default as google_auth_default\n",
        "\n",
        "\n",
        "# @title Input Parameters\n",
        "auto_arima_max_order_list_string = \"2,3,4\" # @param {type:\"string\"}\n",
        "max_time_series_length_list_string = \"512,1024,2048\" # @param {type:\"string\"}\n",
        "google_sheet_name = 'MASE_score_automation' # @param {type:\"string\"}\n",
        "worksheet_name = 'MASE_score_automation' # @param {type:\"string\"}\n",
        "bigquery_project_id = 'bqml_monash' # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Convert the string inputs into lists of integers\n",
        "try:\n",
        "  auto_arima_max_order_list = [int(x.strip()) for x in auto_arima_max_order_list_string.split(',')]\n",
        "  max_time_series_length_list = [int(x.strip()) for x in max_time_series_length_list_string.split(',')]\n",
        "except ValueError as e:\n",
        "   print(f\"Invalid input: {e}. Please provide comma separated integers.\")\n",
        "else:\n",
        "   print(\"auto_arima_max_order_list:\", auto_arima_max_order_list)\n",
        "   print(\"max_time_series_length_list:\", max_time_series_length_list)\n",
        "\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Explicitly request scopes for google_auth_default to ensure BigQuery access is prompted\n",
        "credentials, _ = google_auth_default(scopes=[\n",
        "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "    \"https://www.googleapis.com/auth/drive\", # Drive scope often needed for Sheets access\n",
        "    \"https://www.googleapis.com/auth/bigquery\"\n",
        "]) # Get credentials, ignore default project as we use user_defined_project\n",
        "bigquery.magics.context.project = bigquery_project_id\n",
        "bigquery.magics.context.credentials = credentials\n",
        "from google.cloud.bigquery import magics\n",
        "magics.context.credentials is credentials\n",
        "\n",
        "# initialize gspread client\n",
        "gspread_client = gspread.authorize(credentials)\n",
        "bigquery_client = bigquery.Client(project=bigquery_project_id, credentials=credentials)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "tB0nV-3UXLuj"
      },
      "cell_type": "code",
      "source": [
        "dataset_names = ['australian_electricity_demand', 'pedestrian_counts', 'traffic_hourly', 'bitcoin',\n",
        "                 'covid_deaths', 'nn5_daily', 'saugeenday', 'us_births', 'weather','nn5_weekly',\n",
        "                 'solar_weekly', 'traffic_weekly','tourism_monthly','hospital', 'fred_md', 'cif_2016',\n",
        "                 'tourism_quarterly', 'tourism_yearly']\n",
        "dataset_names_v2 = ['electricity_hourly', 'kdd_cup_2018_without_missing', 'm4_hourly',\n",
        "                    'solar_10_minutes','rideshare_without_missing', 'm4_daily',\n",
        "                    'sunspot_without_missing', 'vehicle_trips_without_missing',\n",
        "                    'temperature_rain_without_missing', 'dominick', 'electricity_weekly',\n",
        "                    'kaggle_web_traffic_weekly', 'm4_weekly', 'car_parts_without_missing',\n",
        "                    'm1_monthly', 'm3_monthly',\n",
        "                    'm4_monthly','m1_quarterly', 'm3_quarterly',\n",
        "                    'm4_quarterly','m3_other', 'm1_yearly', 'm3_yearly', 'm4_yearly']\n",
        "\n",
        "\n",
        "\n",
        "name_to_freq_map = {'australian_electricity_demand':'HALF-HOURLY',\n",
        "                    'pedestrian_counts':'HOURLY',\n",
        "                    'traffic_hourly':'HOURLY',\n",
        "                    'bitcoin':'DAILY',\n",
        "                    'covid_deaths':'DAILY',\n",
        "                    'nn5_daily':'DAILY',\n",
        "                    'saugeenday':'DAILY',\n",
        "                    'us_births':'DAILY',\n",
        "                    'weather':'DAILY',\n",
        "                    'nn5_weekly':'WEEKLY',\n",
        "                    'solar_weekly':'WEEKLY',\n",
        "                    'traffic_weekly':'WEEKLY',\n",
        "                    'tourism_monthly':'MONTHLY',\n",
        "                    'hospital':'MONTHLY',\n",
        "                    'fred_md':'MONTHLY',\n",
        "                    'cif_2016':'MONTHLY',\n",
        "                    'tourism_quarterly':'QUARTERLY',\n",
        "                    'tourism_yearly':'YEARLY',\n",
        "                    'electricity_hourly':'HOURLY',\n",
        "                    'kdd_cup_2018_without_missing':'HOURLY',\n",
        "                    'm4_hourly':'HOURLY',\n",
        "                    'solar_10_minutes':'TEN-MINUTE',\n",
        "                    'rideshare_without_missing':'HOURLY',\n",
        "                    'm4_daily':'DAILY',\n",
        "                    'sunspot_without_missing':'DAILY',\n",
        "                    'vehicle_trips_without_missing':'DAILY',\n",
        "                    'temperature_rain_without_missing':'DAILY',\n",
        "                    'dominick':'WEEKLY',\n",
        "                    'electricity_weekly':'WEEKLY',\n",
        "                    'kaggle_web_traffic_weekly':'WEEKLY',\n",
        "                    'm4_weekly':'WEEKLY',\n",
        "                    'car_parts_without_missing':'MONTHLY',\n",
        "                    'm1_monthly':'MONTHLY',\n",
        "                    'm3_monthly':'MONTHLY',\n",
        "                    'm4_monthly':'MONTHLY',\n",
        "                    'tourism_quarterly':'QUARTERLY',\n",
        "                    'm1_quarterly':'QUARTERLY',\n",
        "                    'm3_quarterly':'QUARTERLY',\n",
        "                    'm4_quarterly':'QUARTERLY',\n",
        "                    'm3_other':'QUARTERLY',\n",
        "                    'm1_yearly':'YEARLY',\n",
        "                    'm3_yearly':'YEARLY',\n",
        "                    'm4_yearly':'YEARLY'}\n",
        "freq_to_horizon_map = {\n",
        "                        'TEN-MINUTE':168,\n",
        "                        'HALF-HOURLY':168,\n",
        "                        'HOURLY':168,\n",
        "                        'DAILY':30,\n",
        "                        'WEEKLY':8,\n",
        "                        'MONTHLY':12,\n",
        "                        'QUARTERLY':30,\n",
        "                        'YEARLY':30\n",
        "                      }\n",
        "\n",
        "freq_to_m_value_map = {\n",
        "                        'TEN-MINUTE':144,\n",
        "                        'HALF-HOURLY':48,\n",
        "                        'HOURLY':24,\n",
        "                        'DAILY':7,\n",
        "                        'WEEKLY':52,\n",
        "                        'MONTHLY':12,\n",
        "                        'QUARTERLY':4,\n",
        "                        'YEARLY':1\n",
        "                      }\n",
        "special_horizon_map = {\n",
        "                        \"australian_electricity_demand\":336,\n",
        "                        \"nn5_daily\": 56,\n",
        "                        \"tourism_monthly\": 24,\n",
        "                        \"m4_weekly\": 13,\n",
        "                        \"m1_monthly\": 18,\n",
        "                        \"m3_monthly\": 18,\n",
        "                        \"m4_monthly\":18\n",
        "                      }\n",
        "DATA_FREQUENCY_EMPTY = 'AUTO_FREQUENCY'\n",
        "DATA_FREQUENCY_YEARLY = 'YEARLY'\n",
        "DATA_FREQUENCY_QUARTERLY = 'QUARTERLY'\n",
        "data_frequency_map = {\n",
        "                      'TEN-MINUTE':DATA_FREQUENCY_EMPTY,\n",
        "                      'HALF-HOURLY':DATA_FREQUENCY_EMPTY,\n",
        "                      'HOURLY':DATA_FREQUENCY_EMPTY,\n",
        "                      'DAILY':DATA_FREQUENCY_EMPTY,\n",
        "                      'WEEKLY':DATA_FREQUENCY_EMPTY,\n",
        "                      'MONTHLY':DATA_FREQUENCY_EMPTY,\n",
        "                      'QUARTERLY':DATA_FREQUENCY_QUARTERLY,\n",
        "                      'YEARLY':DATA_FREQUENCY_YEARLY\n",
        "                    }"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cytdXKDqSeOy"
      },
      "cell_type": "markdown",
      "source": [
        "# Train Model and Evaluate"
      ]
    },
    {
      "metadata": {
        "id": "NBF3UukSXGN1"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_mase(train_df, forecasted_df, test_df, m=1):\n",
        "    errors = []\n",
        "\n",
        "    training_sets_by_id = train_df.groupby('id').agg(\n",
        "        {'value': lambda y: list(y)}\n",
        "    ).to_dict('index')\n",
        "    forecast_sets_by_id = forecasted_df.groupby('id').agg(\n",
        "        {'value': lambda y: list(y)}\n",
        "    ).to_dict('index')\n",
        "    test_sets_by_id = test_df.groupby('id').agg(\n",
        "        {'value': lambda y: list(y)}\n",
        "    ).to_dict('index')\n",
        "\n",
        "    for id in training_sets_by_id:\n",
        "      tsts = np.array(test_sets_by_id[id]['value'])\n",
        "      pts = np.array(forecast_sets_by_id[id]['value'])\n",
        "      length = min(len(tsts), len(pts))\n",
        "      num = np.nanmean(np.abs(tsts[0:length] - pts[0:length]))\n",
        "\n",
        "      original_data = np.array(training_sets_by_id[id]['value'])\n",
        "      interval = m\n",
        "      # If history data is less than frequency, set frequency to 1\n",
        "      if len(original_data) <= m:\n",
        "          interval = 1\n",
        "      denom = np.nanmean(np.abs(original_data[0:-interval] - original_data[interval:]))\n",
        "      if np.isfinite(num / denom):\n",
        "        errors.append(num / denom)\n",
        "    return np.nanmean(errors)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "u3wYEWDXdgOX"
      },
      "cell_type": "code",
      "source": [
        "def setup_google_sheet(google_sheet_name, worksheet_name):\n",
        "  \"\"\"Connects to Google Sheets and prepares the worksheet.\"\"\"\n",
        "  print(\"--- Phase 2: Setup Google Sheet ---\")\n",
        "  try:\n",
        "    spreadsheet = gspread_client.open(google_sheet_name)\n",
        "    sheet = spreadsheet.worksheet(worksheet_name)\n",
        "    print(\n",
        "        f\"Opened existing worksheet: '{worksheet_name}' in spreadsheet\"\n",
        "        f\" '{google_sheet_name}'\"\n",
        "    )\n",
        "\n",
        "    # Check if the sheet is empty to decide whether to add the header\n",
        "    if not sheet.get_all_values():\n",
        "      print(\"Worksheet is empty. Appending header row.\")\n",
        "      header = [\n",
        "          \"Dataset Name\",\n",
        "          \"Max Time Series Length\",\n",
        "          \"Auto ARIMA Max Order\",\n",
        "          \"Model Creation Time\",\n",
        "          \"Frequency\",\n",
        "          \"Horizon\",\n",
        "          \"M Value\",\n",
        "          \"MASE\",\n",
        "          \"MAE\",\n",
        "          \"RMSE\",\n",
        "          \"MAPE\",\n",
        "          \"sMAPE\"\n",
        "      ]\n",
        "      sheet.append_row(header)\n",
        "    else:\n",
        "      print(\"Worksheet already has data. Skipping header row.\")\n",
        "\n",
        "    print(\"--- Google Sheet setup complete. ---\")\n",
        "    return sheet\n",
        "\n",
        "  except gspread.exceptions.SpreadsheetNotFound:\n",
        "    print(\n",
        "        f\"Error: Spreadsheet '{google_sheet_name}' not found. Please\"\n",
        "        \" create it before running the script.\"\n",
        "    )\n",
        "    return None\n",
        "  except gspread.exceptions.WorksheetNotFound:\n",
        "    print(\n",
        "        f\"Error: Worksheet '{worksheet_name}' not found in spreadsheet\"\n",
        "        f\" '{google_sheet_name}'. Please create it before running the\"\n",
        "        \" script.\"\n",
        "    )\n",
        "    return None"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "qv5c7cacSkxX"
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "sheet = setup_google_sheet(google_sheet_name, worksheet_name)\n",
        "for dataset in dataset_names:\n",
        "  for max_time_series_length in max_time_series_length_list:\n",
        "    for auto_arima_max_order in auto_arima_max_order_list:\n",
        "      model_name = 'bqml_tutorial.' + dataset + '_max_time_series_length' + str(max_time_series_length) + '_auto_arima_max_order' + str(auto_arima_max_order)\n",
        "      train_dataset_name = 'scalar-vertex-xgcp.foundation_ts.' + dataset + '_train'\n",
        "      test_dataset_name = 'scalar-vertex-xgcp.foundation_ts.' + dataset + '_test'\n",
        "      data_frequency = data_frequency_map[name_to_freq_map[dataset]]\n",
        "\n",
        "      print(\"Using this model name: \" + model_name)\n",
        "      print(\"Using this train dataset name: \" + train_dataset_name)\n",
        "      print(\"Using this test dataset name: \" + test_dataset_name)\n",
        "      print(\"Using this data frequency: \" + str(data_frequency))\n",
        "\n",
        "      train_query = f\"\"\"\n",
        "      CREATE OR REPLACE MODEL\n",
        "        `{model_name}` OPTIONS( MODEL_TYPE='ARIMA_PLUS',\n",
        "          TIME_SERIES_TIMESTAMP_COL='date',\n",
        "          TIME_SERIES_DATA_COL='value',\n",
        "          TIME_SERIES_ID_COL='ts_id',\n",
        "          AUTO_ARIMA_MAX_ORDER = {auto_arima_max_order},\n",
        "          MAX_TIME_SERIES_LENGTH = {max_time_series_length},\n",
        "          DATA_FREQUENCY = '{data_frequency}'\n",
        "          ) AS\n",
        "      SELECT\n",
        "        date,\n",
        "        ts_id,\n",
        "        value\n",
        "      FROM\n",
        "        `scalar-vertex-xgcp.foundation_ts.{dataset}_train`;\n",
        "      \"\"\"\n",
        "      start_time = time.time()\n",
        "      query_job = bigquery_client.query(train_query)\n",
        "      query_job.result()\n",
        "      end_time = time.time()\n",
        "      elapsed_time = end_time - start_time\n",
        "      print(f\"\"\"Model {model_name} created successfully in {elapsed_time} seconds\"\"\")\n",
        "\n",
        "      freq = name_to_freq_map[dataset]\n",
        "      if dataset not in special_horizon_map:\n",
        "        horizon = freq_to_horizon_map[freq]\n",
        "      else:\n",
        "        horizon = special_horizon_map[dataset]\n",
        "      m = freq_to_m_value_map[freq]\n",
        "      print(\"Using this model name: \" + model_name)\n",
        "      print(\"Using this train dataset name: \" + train_dataset_name)\n",
        "      print(\"Using this test dataset name: \" + test_dataset_name)\n",
        "      print(\"Using this freq: \" + freq)\n",
        "      print(\"Using this horizon: \" + str(horizon))\n",
        "      print(\"Using this m: \" + str(m))\n",
        "\n",
        "      forecast_query = f\"\"\"\n",
        "        SELECT\n",
        "          ts_id AS id,\n",
        "          forecast_timestamp AS timestamp,\n",
        "          forecast_value AS value\n",
        "        FROM\n",
        "          ML.FORECAST(MODEL `{model_name}`, STRUCT({horizon} AS horizon, 0.9 AS confidence_level)) ORDER BY id, timestamp\n",
        "          \"\"\"\n",
        "\n",
        "      forecasted_df = bigquery_client.query(forecast_query).to_dataframe()\n",
        "      forecasted_df['timestamp'] = pd.to_datetime(forecasted_df['timestamp'], utc=True)\n",
        "\n",
        "      test_query = f\"\"\"\n",
        "      SELECT\n",
        "        ts_idd AS id,\n",
        "        date AS timestamp,\n",
        "        value\n",
        "      FROM\n",
        "        `{test_dataset_name}`\n",
        "        ORDER BY id, timestamp\n",
        "      \"\"\"\n",
        "\n",
        "      test_df = bigquery_client.query(test_query).to_dataframe()\n",
        "      test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], utc=True)\n",
        "\n",
        "      train_query = f\"\"\"\n",
        "      SELECT\n",
        "        ts_id AS id,\n",
        "        date AS timestamp,\n",
        "        value\n",
        "      FROM\n",
        "        `{train_dataset_name}`\n",
        "      ORDER BY id, timestamp\n",
        "      \"\"\"\n",
        "\n",
        "      train_df = bigquery_client.query(train_query).to_dataframe()\n",
        "      train_df['timestamp'] = pd.to_datetime(train_df['timestamp'], utc=True)\n",
        "\n",
        "      (current_mase) = calculate_mase(train_df, forecasted_df, test_df, m)\n",
        "      print(\"MASE: \" + str(current_mase))\n",
        "\n",
        "      evaluation_query = f\"\"\"\n",
        "      SELECT\n",
        "        AVG(mean_absolute_error) AS MAE,\n",
        "        AVG(root_mean_squared_error) AS RMSE,\n",
        "        AVG(mean_absolute_percentage_error) AS MAPE,\n",
        "        AVG(symmetric_mean_absolute_percentage_error) AS sMAPE,\n",
        "      FROM\n",
        "      ML.EVALUATE(MODEL `{model_name}`,\n",
        "        (\n",
        "        SELECT ts_idd AS ts_id, date, value,\n",
        "        FROM\n",
        "            `{test_dataset_name}`),\n",
        "        STRUCT(TRUE AS perform_aggregation, {horizon} AS horizon))\n",
        "      \"\"\"\n",
        "      evaluation_df = bigquery_client.query(evaluation_query).to_dataframe()\n",
        "      MAE = evaluation_df['MAE'][0]\n",
        "      RMSE = evaluation_df['RMSE'][0]\n",
        "      MAPE = evaluation_df['MAPE'][0]\n",
        "      sMAPE = evaluation_df['sMAPE'][0]\n",
        "      row_to_append = [\n",
        "          dataset,\n",
        "          max_time_series_length,\n",
        "          auto_arima_max_order,\n",
        "          elapsed_time,\n",
        "          freq,\n",
        "          horizon,\n",
        "          m,\n",
        "          current_mase,\n",
        "          MAE,\n",
        "          RMSE,\n",
        "          MAPE,\n",
        "          sMAPE\n",
        "      ]\n",
        "      sheet.append_row(row_to_append)\n",
        "\n",
        "for dataset in dataset_names_v2:\n",
        "  for max_time_series_length in max_time_series_length_list:\n",
        "    for auto_arima_max_order in auto_arima_max_order_list:\n",
        "      model_name = 'bqml_tutorial.' + dataset + '_max_time_series_length' + str(max_time_series_length) + '_auto_arima_max_order' + str(auto_arima_max_order)\n",
        "      train_dataset_name = 'scalar-vertex-xgcp.foundation_ts.' + dataset + '_train_v2'\n",
        "      test_dataset_name = 'scalar-vertex-xgcp.foundation_ts.' + dataset + '_test_v2'\n",
        "      data_frequency = data_frequency_map[name_to_freq_map[dataset]]\n",
        "\n",
        "\n",
        "      print(\"Using this model name: \" + model_name)\n",
        "      print(\"Using this train dataset name: \" + train_dataset_name)\n",
        "      print(\"Using this test dataset name: \" + test_dataset_name)\n",
        "      print(\"Using this data frequency: \" + str(data_frequency))\n",
        "\n",
        "\n",
        "      train_query = f\"\"\"\n",
        "      CREATE OR REPLACE MODEL\n",
        "        `{model_name}` OPTIONS( MODEL_TYPE='ARIMA_PLUS',\n",
        "          TIME_SERIES_TIMESTAMP_COL='ds',\n",
        "          TIME_SERIES_DATA_COL='y',\n",
        "          TIME_SERIES_ID_COL='unique_id',\n",
        "          AUTO_ARIMA_MAX_ORDER = {auto_arima_max_order},\n",
        "          MAX_TIME_SERIES_LENGTH = {max_time_series_length},\n",
        "          DATA_FREQUENCY = '{data_frequency}'\n",
        "          ) AS\n",
        "      SELECT\n",
        "        ds,\n",
        "        y,\n",
        "        unique_id\n",
        "      FROM\n",
        "        `scalar-vertex-xgcp.foundation_ts.{dataset}_train_v2`;\n",
        "      \"\"\"\n",
        "      start_time = time.time()\n",
        "      query_job = bigquery_client.query(train_query)\n",
        "      query_job.result()\n",
        "      end_time = time.time()\n",
        "      elapsed_time = end_time - start_time\n",
        "      print(f\"\"\"Model {model_name} created successfully \"\"\")\n",
        "      freq = name_to_freq_map[dataset]\n",
        "      if dataset not in special_horizon_map:\n",
        "        horizon = freq_to_horizon_map[freq]\n",
        "      else:\n",
        "        horizon = special_horizon_map[dataset]\n",
        "      m = freq_to_m_value_map[freq]\n",
        "      print(\"Using this model name: \" + model_name)\n",
        "      print(\"Using this train dataset name: \" + train_dataset_name)\n",
        "      print(\"Using this test dataset name: \" + test_dataset_name)\n",
        "      print(\"Using this freq: \" + freq)\n",
        "      print(\"Using this horizon: \" + str(horizon))\n",
        "      print(\"Using this m: \" + str(m))\n",
        "\n",
        "      forecast_query = f\"\"\"\n",
        "        SELECT\n",
        "          unique_id AS id,\n",
        "          forecast_timestamp AS timestamp,\n",
        "          forecast_value AS value\n",
        "        FROM\n",
        "          ML.FORECAST(MODEL `{model_name}`, STRUCT({horizon} AS horizon, 0.9 AS confidence_level)) ORDER BY id, timestamp\n",
        "          \"\"\"\n",
        "\n",
        "      forecasted_df = bigquery_client.query(forecast_query).to_dataframe()\n",
        "      forecasted_df['timestamp'] = pd.to_datetime(forecasted_df['timestamp'], utc=True)\n",
        "\n",
        "      test_query = f\"\"\"\n",
        "      SELECT\n",
        "        ds AS timestamp,\n",
        "        y as value,\n",
        "        unique_id as id\n",
        "      FROM\n",
        "        `{test_dataset_name}` ORDER BY id, timestamp\n",
        "      \"\"\"\n",
        "\n",
        "      test_df = bigquery_client.query(test_query).to_dataframe()\n",
        "      test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], utc=True)\n",
        "\n",
        "      train_query = f\"\"\"\n",
        "      SELECT\n",
        "        ds as timestamp,\n",
        "        y as value,\n",
        "        unique_id as id\n",
        "      FROM\n",
        "        `{train_dataset_name}` ORDER BY id, timestamp\n",
        "      \"\"\"\n",
        "\n",
        "      train_df = bigquery_client.query(train_query).to_dataframe()\n",
        "      train_df['timestamp'] = pd.to_datetime(train_df['timestamp'], utc=True)\n",
        "\n",
        "      (current_mase) = calculate_mase(train_df, forecasted_df, test_df, m)\n",
        "      print(\"MASE: \" + str(current_mase))\n",
        "\n",
        "      evaluation_query = f\"\"\"\n",
        "      SELECT\n",
        "        AVG(mean_absolute_error) AS MAE,\n",
        "        AVG(root_mean_squared_error) AS RMSE,\n",
        "        AVG(mean_absolute_percentage_error) AS MAPE,\n",
        "        AVG(symmetric_mean_absolute_percentage_error) AS sMAPE,\n",
        "      FROM\n",
        "      ML.EVALUATE(MODEL `{model_name}`,\n",
        "        (\n",
        "        SELECT unique_id, ds, y,\n",
        "        FROM\n",
        "            `{test_dataset_name}`),\n",
        "        STRUCT(TRUE AS perform_aggregation, {horizon} AS horizon))\n",
        "      \"\"\"\n",
        "      evaluation_df = bigquery_client.query(evaluation_query).to_dataframe()\n",
        "      MAE = evaluation_df['MAE'][0]\n",
        "      RMSE = evaluation_df['RMSE'][0]\n",
        "      MAPE = evaluation_df['MAPE'][0]\n",
        "      sMAPE = evaluation_df['sMAPE'][0]\n",
        "      row_to_append = [\n",
        "          dataset,\n",
        "          max_time_series_length,\n",
        "          auto_arima_max_order,\n",
        "          elapsed_time,\n",
        "          freq,\n",
        "          horizon,\n",
        "          m,\n",
        "          current_mase,\n",
        "          MAE,\n",
        "          RMSE,\n",
        "          MAPE,\n",
        "          sMAPE\n",
        "      ]\n",
        "      sheet.append_row(row_to_append)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "last_runtime": {
        "build_target": "//monitoring/monarch/tools/colab:notebook",
        "kind": "private"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
